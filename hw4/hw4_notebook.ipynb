{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 Trees and Forests\n",
    "\n",
    "Official instructions:\n",
    "\n",
    "https://www.cs.tufts.edu/cs/135/2024f/Assignments/hw4.html\n",
    "\n",
    "This is the *starter code* notebook for Problems 1 and 2, looking at sentiment classification using bag-of-words features of product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the HW4 starter code\n",
    "from pretty_print_sklearn_tree import pretty_print_sklearn_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all data from train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix to path on your local system\n",
    "DATA_DIR = os.path.join(\"../data_product_reviews/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "x_tr_NF.shape: (6346, 7729)\n",
      "y_tr_N.shape : (6346,)\n",
      "mean(y_tr_N) : 0.500\n"
     ]
    }
   ],
   "source": [
    "x_tr_df = pd.read_csv(os.path.join(DATA_DIR, 'x_train.csv.zip'))\n",
    "y_tr_df = pd.read_csv(os.path.join(DATA_DIR, 'y_train.csv'))\n",
    "x_tr_NF = np.minimum(x_tr_df.values, 1.0).copy()\n",
    "y_tr_N = y_tr_df.values[:,0].copy()\n",
    "\n",
    "print(\"Training data\")\n",
    "print(\"x_tr_NF.shape: %s\" % str(x_tr_NF.shape))\n",
    "print(\"y_tr_N.shape : %s\" % str(y_tr_N.shape))\n",
    "print(\"mean(y_tr_N) : %.3f\" % np.mean(y_tr_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data\n",
      "x_va_TF.shape: (792, 7729)\n",
      "y_va_T.shape : (792,)\n",
      "mean(y_va_T) : 0.490\n"
     ]
    }
   ],
   "source": [
    "x_va_df = pd.read_csv(os.path.join(DATA_DIR, 'x_valid.csv.zip'))\n",
    "y_va_df = pd.read_csv(os.path.join(DATA_DIR, 'y_valid.csv'))\n",
    "\n",
    "x_va_TF = np.minimum(x_va_df.values, 1.0).copy()\n",
    "y_va_T = y_va_df.values[:,0].copy()\n",
    "\n",
    "print(\"Validation data\")\n",
    "print(\"x_va_TF.shape: %s\" % str(x_va_TF.shape))\n",
    "print(\"y_va_T.shape : %s\" % str(y_va_T.shape))\n",
    "print(\"mean(y_va_T) : %.3f\" % np.mean(y_va_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heldout Test data\n",
      "x_te_TF.shape: (793, 7729)\n",
      "y_te_T.shape : (793,)\n",
      "mean(y_te_T) : 0.515\n"
     ]
    }
   ],
   "source": [
    "x_te_df = pd.read_csv(os.path.join(DATA_DIR, 'x_test.csv.zip'))\n",
    "y_te_df = pd.read_csv(os.path.join(DATA_DIR, 'y_test.csv'))\n",
    "\n",
    "x_te_TF = np.minimum(x_te_df.values, 1.0).copy()\n",
    "y_te_T = y_te_df.values[:,0].copy()\n",
    "\n",
    "print(\"Heldout Test data\")\n",
    "print(\"x_te_TF.shape: %s\" % str(x_te_TF.shape))\n",
    "print(\"y_te_T.shape : %s\" % str(y_te_T.shape))\n",
    "print(\"mean(y_te_T) : %.3f\" % np.mean(y_te_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabulary as a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = x_tr_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "great\n",
      "time\n",
      "book\n",
      "don't\n",
      "work\n",
      "i_have\n",
      "read\n",
      "...\n",
      "never_get\n",
      "i'd_like\n",
      "loves_it\n",
      "an_author\n",
      "nomin\n",
      "could_give\n",
      "bad_but\n",
      "gap\n"
     ]
    }
   ],
   "source": [
    "for word in vocab_list[:8]:\n",
    "    print(word)\n",
    "print(\"...\")\n",
    "for word in vocab_list[-8:]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack training and validation sets into big arrays (so we can use sklearn's hyperparameter search tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xall_LF = np.vstack([x_tr_NF, x_va_TF])\n",
    "yall_L = np.hstack([y_tr_N, y_va_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indicators_L = np.hstack([\n",
    "    -1 * np.ones(y_tr_N.size), # -1 means never include this example in any test split\n",
    "    0  * np.ones(y_va_T.size), #  0 means include in the first test split (we count starting at 0 in python)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splitter object using Predefined Split\n",
    "# Will be used later by all hyperparameter searches\n",
    "\n",
    "my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Decision Trees\n",
    "\n",
    "## 1A: Train a simple tree with depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree = sklearn.tree.DecisionTreeClassifier(\n",
    "    criterion='gini', \n",
    "    max_depth=3, min_samples_split=2, min_samples_leaf=1, \n",
    "    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fit the tree** \n",
    "\n",
    "**TODO Train on the training set** in the next coding cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# simple_tree.fit(FIXME, FIXME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Figure 1: Print Tree** \n",
    "\n",
    "Use a helper function from the starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# call pretty_print_sklearn_tree on simple_tree, providing feature_names=vocab_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B : Find best Decision Tree with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the default predictor\n",
    "# Any hyperparameters here may be overridden by the hyperparameter grid\n",
    "\n",
    "base_tree = sklearn.tree.DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    min_samples_split=2, min_samples_leaf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_hyperparameter_grid_by_name = dict(\n",
    "    max_depth=[2, 8, 32, 128],\n",
    "    min_samples_leaf=[1, 3, 9],\n",
    "    random_state = [101],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Build the Grid Search** in the next coding cell\n",
    "\n",
    "Follow the 1B instructions carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid_searcher = None # TODO fixme use GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the search!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_sec = time.time()\n",
    "# TODO call me: tree_grid_searcher.fit(xall_LF, yall_L)\n",
    "elapsed_time_sec = time.time() - start_time_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataframe of results\n",
    "\n",
    "Move the results of grid search into a nice pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_search_results_df = pd.DataFrame(tree_grid_searcher.cv_results_).copy()\n",
    "print(\"Grid search of %3d configurations done after %6.1f sec\" % (\n",
    "    tree_search_results_df.shape[0], elapsed_time_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display search results\n",
    "\n",
    "This block will make a pretty printed table of the results of your grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 4)\n",
    "tree_keys = ['param_max_depth', 'param_min_samples_leaf']\n",
    "tree_search_results_df.sort_values(tree_keys, inplace=True)\n",
    "tree_search_results_df[tree_keys + ['mean_train_score', 'mean_test_score', 'rank_test_score', 'mean_fit_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing a dict of the best hyperparameters\")\n",
    "try:\n",
    "    print(tree_grid_searcher.best_params_)\n",
    "except AttributeError:\n",
    "    print(\"TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the best decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Build the Best Tree on the training set** in the next coding cell\n",
    "\n",
    "This is necessary so you have the specific best performing tree in your workspace.\n",
    "\n",
    "Although you fit many trees in the search, they were not stored, so we need to recreate the best one.\n",
    "\n",
    "Hint: Just feed the best hyperparameters as keyword args to construct the tree and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = base_tree # TODO call set_params using the best_params_ found by your searcher\n",
    "best_tree.fit(x_tr_NF, y_tr_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the best decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_sklearn_tree(best_tree, feature_names=vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A: Train a random forest with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_forest = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_features='sqrt',\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the forest\n",
    "\n",
    "**TODO Train on the training set** in the next coding cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FIXME call fit, like: `simple_forest.fit(None, None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B & Table 2: Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2\n",
    "**Sample Output** (Feel free to print all words and organize them in any software)\n",
    "\n",
    "|**Important Words**|**Unimportant Words**|\n",
    "|:-:|:-:|\n",
    "|I1 |  U1  |\n",
    "|I2 |  U2  |\n",
    "|I3 |  U3  |\n",
    "|I4 |  U4  |\n",
    "|I5 |  U5  |\n",
    "|I6 |  U6  |\n",
    "|I7 |  U7  |\n",
    "|I8 |  U8  |\n",
    "|I9 |  U9  |\n",
    "|I0 |  U0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2C: Best Random Forest via grid search\n",
    "\n",
    "Follow the instructions and using what you learn in 1B to finish this step.\n",
    "\n",
    "This block might take 2-10 minutes. (Takes about 2 min on staff Macbook laptops.)\n",
    "\n",
    "If yours runs significantly longer, try this out on Google Colab instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_forest = sklearn.ensemble.RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=16,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_hyperparameter_grid_by_name = dict(\n",
    "    max_features=[3, 10, 33, 100, 333],\n",
    "    max_depth=[16, 32],\n",
    "    min_samples_leaf=[1],\n",
    "    n_estimators=[100],\n",
    "    random_state=[101],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO construct a GridSearchCV object like you did above.\n",
    "\n",
    "forest_searcher = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataframe of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the best random forest using the best hyperparameters found in 2B \n",
    "\n",
    "This is necessary so you have the specific best performing forest in your workspace.\n",
    "\n",
    "Train *only* on training set (do not merge train and valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3: Comparison of methods on the bag-of-words to sentiment classification task.\n",
    "\n",
    "Please report **balanced accuracy** on the train, valid, and test sets, to 3 digits of precision\n",
    "\n",
    "**Sample Output** (Feel free to print all values and organize them by hand)\n",
    "\n",
    "|**method**|**max depth**|**num trees**|**train BAcc**|**valid BAcc**|**test BAcc**|\n",
    "|:-|:-:|:-:|:-:|:-:|:-:|\n",
    "|simple Tree\t| 1 | 1 | 0.123\t|0.456\t|0.890|\n",
    "|best Tree\t|1 | 1 | 0.123\t|0.456\t|0.890|\n",
    "|simple RandomForest\t|1 | 1 | 0.123\t|0.456\t|0.890|\n",
    "|best RandomForest\t|1 | 1 | 0.123\t|0.456\t|0.890|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
